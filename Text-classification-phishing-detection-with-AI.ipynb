{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5niycHbUYsKC"
      },
      "outputs": [],
      "source": [
        "# module imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import random\n",
        "import re\n",
        "\n",
        "# model imports\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# processing imports\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from termcolor import colored\n",
        "nltk.download('stopwords')\n",
        "stopwords.words(\"english\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras.layers import Dense, LSTM, MaxPool1D, Flatten, Dropout, Conv1D, Activation, Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Embedding, Dropout, Flatten, Bidirectional, SimpleRNN\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# import pipeline and SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIgjAw2vYunE"
      },
      "outputs": [],
      "source": [
        "trd = pd.read_csv('a.csv')\n",
        "ted = pd.read_csv('CEAS_08.csv')\n",
        "tcd = pd.read_csv('b.csv', encoding='utf-8', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MkcNFQoYwfo"
      },
      "outputs": [],
      "source": [
        "trd = trd.drop(columns=['Unnamed: 0'])\n",
        "trd = trd.rename(columns = {\"Email Text\" : \"Text\", \"Email Type\" : \"label\"})\n",
        "trd['label'].replace({'Safe Email': 0, 'Phishing Email': 1}, inplace=True)\n",
        "trd.astype({'label': 'int64'}).dtypes\n",
        "trd.info()\n",
        "trd.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhC6Y3oXYwdZ"
      },
      "outputs": [],
      "source": [
        "ted[\"Text\"] = ted[\"subject\"] + \" \" + ted[\"body\"]\n",
        "ted = ted.drop(columns=['sender', 'receiver', 'date', 'subject', 'body', 'urls'])\n",
        "ted = ted[['Text', 'label']]\n",
        "ted.info()\n",
        "ted.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5gnql-lYwY8"
      },
      "outputs": [],
      "source": [
        "tcd[\"Text\"] = tcd[\"subject\"] + \" \" + tcd[\"body\"]\n",
        "tcd = tcd.drop(columns=['sender', 'receiver', 'date', 'subject', 'body', 'urls'])\n",
        "tcd = tcd[['Text', 'label']]\n",
        "tcd.info()\n",
        "tcd.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q01QeofebAGn"
      },
      "source": [
        "Combined datasets (testing machines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxj7qDSuY1cM"
      },
      "outputs": [],
      "source": [
        "frames = [trd, ted, tcd]\n",
        "data_1 = pd.concat(frames)\n",
        "data_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ArRgaArbNmh"
      },
      "source": [
        "2 training dataset and 1 testing dataset (used for results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr9_Ijbgb5ZX"
      },
      "outputs": [],
      "source": [
        "frames = [trd, tcd]\n",
        "data_2 = pd.concat(frames)\n",
        "data_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sc_Hj0CbSIg"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess(Text):\n",
        "    Text = re.sub(r'[^\\w\\s]', ' ', str(Text).lower()).strip()\n",
        "    return Text\n",
        "\n",
        "# Preprocess training and test data\n",
        "data_2['Text'] = data_2['Text'].apply(preprocess)\n",
        "ted['Text'] = ted['Text'].apply(preprocess)\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_df=1.0,\n",
        "    min_df=1,\n",
        "    max_features=6000\n",
        ")\n",
        "\n",
        "# Fit the vectorizer on the training data and transform both datasets\n",
        "X_train_tfidf = vectorizer.fit_transform(data_2['Text']).toarray()\n",
        "X_test_tfidf = vectorizer.transform(ted['Text']).toarray()\n",
        "\n",
        "# Reshape for LSTM and CNN\n",
        "X_train_tfidf = X_train_tfidf.reshape(X_train_tfidf.shape[0], X_train_tfidf.shape[1], 1)\n",
        "X_test_tfidf = X_test_tfidf.reshape(X_test_tfidf.shape[0], X_test_tfidf.shape[1], 1)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "y_train = np.array(data_2['label'])\n",
        "y_test = np.array(ted['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2xTx_afciN0"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN_uDaNzWjkC"
      },
      "outputs": [],
      "source": [
        "# 1. ANN Model\n",
        "def ann():\n",
        "    model = Sequential(name=\"ANN\")\n",
        "    model.add(Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#2. RNN Model\n",
        "def rnn():\n",
        "    model = Sequential(name=\"RNN\")\n",
        "    model.add(SimpleRNN(64, input_shape=(X_train_tfidf.shape[1], 1), return_sequences=False))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. CNN Model\n",
        "def cnn():\n",
        "    model = Sequential(name=\"CNN\")\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_tfidf.shape[1], 1)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 4. CNN-LSTM Model\n",
        "def cnn_lstm():\n",
        "    model = Sequential(name=\"CNN-LSTM\")\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_tfidf.shape[1], 1)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model\n",
        "models = {\n",
        "    'ANN': ann(),\n",
        "    'RNN': rnn(),\n",
        "    'CNN': cnn(),\n",
        "    'CNN-LSTM': cnn_lstm()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkJxAhfLd59w"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOubJCrhqnU5"
      },
      "outputs": [],
      "source": [
        "# Define function to evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    # Train and evaluate all models\n",
        "for model, model in models.items():\n",
        "    print(f\"\\nTraining {model} model...\")\n",
        "\n",
        "    # Fit the model and store the history object\n",
        "    history = model.fit(\n",
        "        X_train_tfidf, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=64,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    print(f\"Evaluating {model} model...\")\n",
        "    evaluate_model(model, X_test_tfidf, y_test)\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    trainScore = model.evaluate(X_train_tfidf, y_train, verbose=0)\n",
        "    print(\"Our accuracy is %{}\".format(trainScore[1] * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt7ZnKlqefqF"
      },
      "source": [
        "Plotting the graph and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrS4N-6jF-Be"
      },
      "outputs": [],
      "source": [
        "# Function to plot training and validation metrics\n",
        "def plot_graphs(var1, var2, string, metrics):\n",
        "    metrics[[var1, var2]].plot()\n",
        "    plt.title('Model: Training and Validation ' + string)\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([var1, var2])\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm):\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    fig, ax = plt.subplots(figsize=(9, 7))\n",
        "    disp.plot(ax=ax, cmap=\"viridis\", colorbar=True)\n",
        "\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Loop through each model\n",
        "for model_name, model in models.items():\n",
        "    # Print the model name for clarity\n",
        "    print(f\"\\nEvaluating model: {model_name}\\n{'='*40}\")\n",
        "\n",
        "    # Convert history to DataFrame for further analysis\n",
        "    metrics = pd.DataFrame(history.history)\n",
        "\n",
        "    # Rename columns to match desired format\n",
        "    metrics.rename(columns={'loss': 'Training_Loss',\n",
        "                            'accuracy': 'Training_Accuracy',\n",
        "                            'val_loss': 'Validation_Loss',\n",
        "                            'val_accuracy': 'Validation_Accuracy'}, inplace=True)\n",
        "\n",
        "    # Print before plotting loss\n",
        "    print(f\"Plotting loss graphs for {model_name}\")\n",
        "    plot_graphs('Training_Loss', 'Validation_Loss', 'Loss', metrics)\n",
        "\n",
        "    # Print before plotting accuracy\n",
        "    print(f\"Plotting accuracy graphs for {model_name}\")\n",
        "    plot_graphs('Training_Accuracy', 'Validation_Accuracy', 'Accuracy', metrics)\n",
        "\n",
        "    # Predict and create confusion matrix\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    y_pred = (y_pred > 0.5)  # Adjust threshold if necessary\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print message before confusion matrix plot\n",
        "    print(f\"Plotting confusion matrix for {model_name}\")\n",
        "    plot_confusion_matrix(cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNFBR0Zxg1ML"
      },
      "source": [
        "Machine Learning: Logistic Regression,\n",
        "K-Neighbors,\n",
        "Random Forest,\n",
        "SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMaIw3-MCZ0O"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess(Text):\n",
        "    Text = re.sub(r'[^\\w\\s]', ' ', str(Text).lower()).strip()\n",
        "    return Text\n",
        "\n",
        "# Preprocess training and test data\n",
        "data_2['Text'] = data_2['Text'].apply(preprocess)\n",
        "ted['Text'] = ted['Text'].apply(preprocess)\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_df=1.0,\n",
        "    min_df=1,\n",
        "    max_features=5000\n",
        ")\n",
        "\n",
        "# Fit the vectorizer on the training data and transform both datasets\n",
        "X_train_tfidf = vectorizer.fit_transform(data_2['Text']).toarray()\n",
        "X_test_tfidf = vectorizer.transform(ted['Text']).toarray()\n",
        "\n",
        "# Convert labels to numpy array\n",
        "y_train = np.array(data_2['label'])\n",
        "y_test = np.array(ted['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftjesZg0IM8c"
      },
      "outputs": [],
      "source": [
        "# Initialize classifiers\n",
        "models = {\n",
        "    \"SVC\": SVC(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"K-Neighbors\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Train, predict, and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Display classification report\n",
        "    print(f\"\\n{model_name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{model_name} Accuracy: {accuracy:.2f} %\")\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm',\n",
        "                xticklabels=['Normal', 'Phishing'], yticklabels=['Normal', 'Phishing'])\n",
        "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
